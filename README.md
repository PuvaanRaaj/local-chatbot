# ğŸ§  LLM Chatbot (Code Review, Generation, Debugging, and More)

This project is a lightweight, developer-friendly chatbot powered by **Docker Model Runner** (e.g. LLaMA 3.2 or other local models), styled like ChatGPT, with support for:

* ğŸš  **Code Review**
* âš™ï¸ **Code Generation**
* ğŸ’¡ **General Q\&A**
* ğŸ **Debugging**
* ğŸš€ **Optimization**

It runs fully locally via **Docker + Flask**, with markdown rendering, streaming response, copy buttons, and multiple modes.

---

## ğŸ“¦ Features

* ğŸŒ— Dark-mode responsive UI with Tailwind
* ğŸ“ Markdown + code block rendering
* ğŸ“¤ Upload file support (+ icon)
* ğŸ“¥ Streamed assistant typing effect
* ğŸ”Œ Multiple endpoints and prompts (review, generate, debug...)
* ğŸ³ Runs entirely on Docker (LLM + app)

---

## ğŸ“‚ Project Structure

```
llm-chatbot/
â”œâ”€â”€ app.py                # Flask entry
â”œâ”€â”€ routes/              # Route handlers
â”‚   â””â”€â”€ chat_routes.py
â”œâ”€â”€ services/            # Chat logic
â”‚   â””â”€â”€ llama_runner.py
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html       # Mode selector
â”‚   â””â”€â”€ chat.html        # Chat UI
â”œâ”€â”€ .env                 # Copied from .env.example
â”œâ”€â”€ .env.example         # Sample environment file
â”œâ”€â”€ requirements.txt
â””â”€â”€ demo/
    â””â”€â”€ <your-screenshots-here>.png
```

---

## ğŸš€ Getting Started

### 1. Prerequisites

* Docker Desktop âœ…
* Docker Model Runner plugin âœ…

### 2. Clone This Repo

```bash
git clone https://github.com/yourname/llm-chatbot
cd llm-chatbot
cp .env.example .env
```

### 3. Install Docker Model Runner (Only Once)

```bash
docker extension install docker/model:latest
```

Then enable TCP support:

```bash
docker model install-runner --tcp 12434
```

Or in Docker Desktop:

1. Go to **Settings > Features in Development**
2. Enable **Model Runner TCP support**

ğŸ“· Place screenshot in `demo/docker-model-runner-enable.png`

### 4. Pull Your Model

```bash
docker model pull ai/llama3.2:latest
```

ğŸ§  You may use any compatible model such as `meta/llama3`, `openchat/openchat`, or `mistralai/mistral`.

---

## ğŸ›  Run It (Local Dev)

Use Docker to start the chatbot container:

```bash
docker build -t llm-chat-ui .
docker run --rm -it -v "$PWD":/app -p 12345:12345 llm-chat-ui
```

Then visit: [http://localhost:12345](http://localhost:12345)

---

## ğŸ”§ .env (Optional)

`.env.example`:

```env
FLASK_DEBUG=1
```

Copy it:

```bash
cp .env.example .env
```

---

## ğŸ“„ requirements.txt

```txt
flask
requests
python-dotenv
```

---

## ğŸ“· Screenshots

* `demo/chat-ui-streaming.png` â€“ main UI
* `demo/chat-ui-modes.png` â€“ mode selector
* `demo/docker-model-runner-enable.png` â€“ TCP setup

---

## âœ… Features to Explore

* [ ] Add Chat History (localStorage?)
* [ ] Auth or Login (JWT, sessions)
* [ ] Save/Share prompts
* [ ] WebSocket for true streaming

---

## ğŸ§  Credits

* Built by \[Puvaan Raaj]
* Powered by Docker Model Runner

---

## ğŸ“œ License

MIT. Use freely. Attribution appreciated.
